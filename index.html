<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A MICCAI publication about vision-language models in medical imaging: These models struggle to identify the relative positions of anatomical structures, tending to ignore visual input in favour of prior knowledge encoded in the language component.">
  <meta name="keywords" content="Vision Language Models, Multimodal Large Language Models, Medical Imaging, CT, Benchmark Dataset, Relative Positions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Your other Left! – Vision-Language Models Fail to Identify Relative Positions in Medical Images</title>

  <!-- Google Analytics code removed as requested -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- ############################################################
     #Picture in Tab: [ToDo: Freistellen]
     ############################################################ -->
  <link rel="icon" href="./static/images/UniUlm.PNG">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- ############################################################
     #Authors and Affiliations: [ToDo: Links]
     ############################################################ -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-2 publication-title">
            Your <em>other</em> Left! <br>
            Vision-Language Models Fail <br>
            to Identify Relative Positions in Medical Images
          </h1>

          <div class="is-size-5">
            MICCAI 2025
          </div>

          <br>

          <div class="is-size-8 publication-authors">
            <span class="author-block">
              <a href="https://viscom.uni-ulm.de/members/daniel-wolf/">Daniel Wolf</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/heiko-hillenhagen/">Heiko Hillenhagen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.uniklinik-ulm.de/radiologie-diagnostische-und-interventionelle/team.html">Billurvan Taskin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://a13x.io/">Alex Bäuerle</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.uniklinik-ulm.de/radiologie-diagnostische-und-interventionelle/team/prof-dr-m-beer.html">Meinrad Beer</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.uniklinik-ulm.de/radiologie-diagnostische-und-interventionelle/schwerpunkte-sektionen/sektion-experimentelle-radiologie.html">Michael Götz</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="https://viscom.uni-ulm.de/members/timo-ropinski/">Timo Ropinski</a><sup>1,*</sup>
            </span>
          </div>

          <div class="is-size-10">
            <em>* Shared Last Authorship</em>
          </div>

          <br>

          <div class="is-size-8 publication-authors">
            <span class="author-block">
              <sup>1</sup><a href="https://viscom.uni-ulm.de/">Visual Computing Group, Ulm University</a>,</span>
            <span class="author-block">
              <sup>2</sup><a href="https://www.uniklinik-ulm.de/en/radiology-diagnostic-and-interventional-radiology.html">Clinic for Radiology, Ulm University Medical Center</a>,</span>
            <span class="author-block">
              <sup>3</sup><a href="https://axi.om/">Axiom Bio</a>
            </span>
          </div>

          <!-- ############################################################
               # Top Buttons  (Paper | arXiv | Video | Code | Data)
               ############################################################ -->
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Paper (dropdown) -->
              <div class="link-block dropdown" id="paper-dropdown">
                <!-- toggle -->
                <a class="external-link button is-normal is-rounded is-dark"
                   role="button" aria-haspopup="true" aria-controls="dropdown-menu-paper">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                  <span class="icon is-small"><i class="fas fa-angle-down"></i></span>
                </a>
                <!-- menu -->
                <div class="dropdown-menu" id="dropdown-menu-paper" role="menu">
                  <div class="dropdown-content">
                    <a href="https://wolfda95.github.io/your_other_left/"
                       class="dropdown-item" target="_blank" rel="noopener">
                      MICCAI&nbsp;Proceedings
                    </a>
                    <a href="https://arxiv.org/pdf/2508.00549"
                       class="dropdown-item" target="_blank" rel="noopener">
                      arXiv&nbsp;PDF
                    </a>
                  </div>
                </div>
              </div>

              <!-- Code -->
              <span class="link-block">
                <a href="https://github.com/Wolfda95/MIRP_Benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              
              <!-- Data -->
              <span class="link-block">
                <a href="https://github.com/Wolfda95/MIRP_Benchmark/tree/main/1_dataset_guide"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-angle-down"></i></span>
                  <span>Data</span>
                </a>
              </span>
          
              <!-- Video -->
              <span class="link-block">
                <a href="https://wolfda95.github.io/your_other_left/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-youtube"></i></span>
                  <span>Video</span>
                </a>
              </span>

              <!-- Connect on LinkedIn -->
              <div class="link-block dropdown" id="data-dropdown">
                <!-- toggle -->
                <a class="external-link button is-normal is-rounded is-dark"
                   role="button" aria-haspopup="true" aria-controls="dropdown-menu-data">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>Connect</span>
                  <span class="icon is-small"><i class="fas fa-linkedin"></i></span>
                </a>
                <!-- menu -->
                <div class="dropdown-menu" id="dropdown-menu-data" role="menu">
                  <div class="dropdown-content">
                    <a href="https://www.linkedin.com/in/wolf-daniel/"
                       class="dropdown-item" target="_blank" rel="noopener">
                      Daniel&nbsp;Wolf
                    </a>
                    <a href="https://www.linkedin.com/in/heiko-hillenhagen/"
                       class="dropdown-item" target="_blank" rel="noopener">
                      Heiko&nbsp;Hillenhagen
                    </a>
                  </div>
                </div>
              </div>
          
            </div>
          </div>
          <!-- /Top Buttons -->
        </div>
      </div>
    </div>
  </div>
</section>



<!-- ############################################################
     #Top Box: [ToDo: Text]
     ############################################################ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="box" style="background-color:#9546e3;color:#ffffff;">
      <h2 class="title is-4 has-text-white"><strong>Main Findings:</strong></h2>
        <div class="content">
          <p>
            We show that Vision–Language Models like GPT4o, Pixtral, Llama 3.2, Gemma 3, …
          </p>
          <ul>
            <li>fail to identify relative positions of anatomical structures in medical images.</li>
            <li>prioritize memorized knowledge within the language part over actual image content.</li>
          </ul>
  
          <p class="mt-4">
            To address these limitations, we introduce MIRP: An open benchmark dataset for evaluating relative-positioning tasks in medical imaging.
          </p>
        </div>
    </div>

    <!-- Podcast player -->
    <div class="mt-4">
      <audio controls style="width: 100%;">
        <source src="static/images/podcast.mp3" type="audio/mpeg">
        Your browser doesn’t support the audio element.
      </audio>
      <p><em>This is a podcast about the paper generated with NotebookLM</em></p>
    </div>
    
  </div>
</section>

<!-- ############################################################
     # Research Questions (white background, full-width)
     ############################################################ -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- RQ1 (normal size, bold) -->
    <p class="is-size-5 has-text-weight-bold">
      Research Question 1: Can current top-tier VLMs accurately determine relative positions of anatomical structures in radiological images?
    </p>
    <p>
      We introduce a dataset of over 4,000 CT slices, each paired with a question about the relative position of two visible anatomical structures. The slices are randomly flipped and rotated.
    </p>
    <figure class="image mt-4">
      <img src="static/images/1.PNG" alt="Overview for RQ1">
    </figure>
    <p class="mt-2">→ All models fail at this basic task</p>

    <!-- RQ2 (normal size, bold) -->
    <p class="is-size-5 has-text-weight-bold mt-6">
      Research Question 2: Can visual markers improve VLMs’ ability to determine relative positions in radiological images?
    </p>
     <p>
       Given the high accuracy of segmentation models, we try to support the vision-language models by placing visual markers on the two relevant structures, derived from segmentation outputs. 
    </p>
    <figure class="image mt-4">
      <img src="static/images/2.PNG" alt="Overview for RQ2">
    </figure>
    <p class="mt-2">→ Markers bring no significant improvements</p>

    <!-- RQ3 (normal size, bold) -->
    <p class="is-size-5 has-text-weight-bold mt-6">
      Research Question 3: Do VLMs prioritize memorized anatomical knowledge over visual input when determining relative positions in radiological images?
    </p>
    <p class="has-text-weight-bold mt-5">
      (1) Do VLMs’ responses align with standard human anatomy?
    </p>
    <p>
      Relative positions of anatomical structures can be provided in two ways: (a) based on the specific orientation of the provided rotated/flipped image, (b) based on the typical anatomical positions of the structures in standard human anatomy. For example, the correct answer to the question “Is the liver to the right of the stomach?” would be yes, based on standard human anatomy, since the liver is to the right of the stomach in most humans. But based on the provided image, the correct answer might be no. We evaluate the VLMs’ answers to the left/right questions in two ways: (a) correctness based on the provided rotated/flipped image (same evaluation as in RQ1) and (b) correctness based on how the structures are normally positioned in standard anatomy.
    </p>
    <figure class="image mt-4">
      <img src="static/images/31.PNG" alt="Overview for RQ2">
    </figure>
    <p class="has-text-weight-bold mt-5">
      (2) How well do VLMs perform when prior knowledge cannot be used?
    </p>
    <p>
      Fot this evaluation we modify the questions so that models are given only visual markers with no reference to anatomical structures. 
    </p>
    <figure class="image mt-4">
      <img src="static/images/4.PNG" alt="Overview for RQ2">
    </figure>
    <p class="mt-4">
      These evaluations test our hypothesis that VLMs rely more on prior anatomical knowledge than on image content in two ways:
    </p> 
    <p class="mt-2"> (1) When VLMs have access to anatomical names, GPT-4o, Pixtral, and Gemma3 produce more anatomically correct answers than image-based correct answers. This indicates a reliance on prior knowledge, as an anatomically correct answer can only stem from prior knowledge within the language part.</p>
    <p class="mt-2"> (2) When anatomical names are removed, forcing the models to rely solely on image content, GPT-4o, Pixtral, and Gemma3 achieve high accuracies when evaluated based on the image view.</p>
    <!-- Closing sentence -->
    <p class="mt-2">→
      This implies that
      <mark>
         GPT-4o, Pixtral, and Gemma3 are generally capable of detecting relative positions, but the use of medical terminology in the question shifts their responses from visual reasoning to recalling typical anatomical knowledge embedded in their language components. 
      </mark>
    </p>
    <p class="mt-2"> Multiple prompt variations were evaluated prior to the question (e.g., “Ignore anatomical correctness; focus solely on what the image shows”), yet none yielded measurable improvements. </p>

    <!-- CQ (normal size, bold) -->
    <p class="is-size-5 has-text-weight-bold mt-6">
      Why This Matters:
    </p>
    <p>
      Imagine a radiology department where Vision-Language Models assist with complex tasks such as generating radiology reports or supporting surgical planning. If these models fail at the fundamental task of identifying the relative positions of anatomical structures in medical images, the consequences could be severe: wrong-level spine surgeries, wrong-side procedures, etc.. Moreover, if VLMs rely on memorized anatomical norms encoded in their language components rather than actual visual evidence, they risk critical misdiagnoses in cases where anatomy deviates from typical patterns, such as situs inversus, post-surgical changes, or tumor-induced displacements.
    </p>
    <figure class="image mt-4">
      <img src="static/images/5.PNG" alt="Overview for RQ2">
    </figure>
    <p class="mt-2">→ These limitations highlight the need for further research before VLMs can be safely and reliably integrated into radiological workflows.</p>

    <!-- Video -->
    <h2 class="title is-3 has-text-weight-bold mt-6">
      Watch the Video for more details
    </h2>
    <p>
      Coming soon
    </p>

    
  </div>
</section>

<!-- ############################################################
     #How to use the code: [ToDo: Text + Bilder]
     ############################################################ -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">The MIRP Benchmark Dataset and Code</h2>
        <p>To facilitate future research we introduce the MIRP (Medical Imaging Relative Positioning) benchmark dataset.</p>
        <p class="mt-2">MIRP contains the datasets for all Research Questions (RQ1, RQ2, RQ3). We always include a folder with the CT slices and a json file with the questions-answer pairs for each slice. </p>
        <p class="mt-2">For more information on how to use the dataset visit our GitHub page or watch the video below</p>
      
        <p class="is-size-5 has-text-weight-bold mt-6">
        Video on how to use the Dataset and Code
        </p>
        <p>
        Coming soon
        </p>
      
    </div>
  </div>
</section>

<!-- ############################################################
     #Cite: [ToDo: Richtige Citation]
     ############################################################ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@inproceedings{wolf2025medvlms,
  title={Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images},
  author={Wolf, Daniel and Hillenhagen, Heiko and Taskin, Billurvan and B{\"a}uerle, Alex and Beer, Meinrad and G{\"o}tz, Michael and Ropinski, Timo},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention -- MICCAI},
  year={2025},
  organization={Springer Nature Switzerland}
  doi={tba}
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>.
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    // toggle any dropdown when clicked
    document.querySelectorAll('.link-block.dropdown').forEach(dd => {
      dd.addEventListener('click', e => {
        e.stopPropagation();
        dd.classList.toggle('is-active');
      });
    });

    // close all dropdowns when clicking anywhere else
    document.addEventListener('click', () => {
      document.querySelectorAll('.link-block.dropdown').forEach(dd => {
        dd.classList.remove('is-active');
      });
    });
  });
</script>


</body>
</html>
